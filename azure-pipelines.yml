# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

pool:
  vmImage: 'ubuntu-latest'

variables:
- group: EnvironmentVariables

steps:
- script: echo $(keyVaultName)
  displayName: 'Show variable(s)'

#- script: az --version
#  displayName: 'Show Azure CLI version'
  
#- script: az upgrade --yes
#  displayName: 'Upgrade Azure CLI version if needed'

#- task: AzureCLI@2
#  inputs:
#    azureSubscription: 'ARM'
#    scriptType: 'pscore'
#    scriptLocation: 'scriptPath'
#    scriptPath: '01 - ResourceGroup.ps1'
#    arguments: '-rgName $(rgName) -defaultLocation $(defaultLocation)'

# - task: AzureCLI@2
#   inputs:
#     azureSubscription: 'ARM'
#     scriptType: 'pscore'
#     scriptLocation: 'scriptPath'
#     scriptPath: '02 - StorageAccount.ps1'
#     arguments: '-rgName $(rgName) -storageAccountName $(storageAccountName)'
# - task: AzureCLI@2
#   inputs:
#     azureSubscription: 'ARM'
#     scriptType: 'pscore'
#     scriptLocation: 'scriptPath'
#     scriptPath: '03 - KeyVault.ps1'
#     arguments: '-rgName $(rgName) -keyVaultName $(keyVaultName)'
# - task: AzureCLI@2
#   inputs:
#     azureSubscription: 'ARM'
#     scriptType: 'pscore'
#     scriptLocation: 'scriptPath'
#     scriptPath: '04 - AzureDataFactory.ps1'
#     arguments: '-rgName $(rgName) -ADFName $(ADFName)'
# - task: AzureCLI@2
#   inputs:
#     azureSubscription: 'ARM'
#     scriptType: 'pscore'
#     scriptLocation: 'scriptPath'
#     scriptPath: '05 - AzureDatabricks.ps1'
#     arguments: '-rgName $(rgName) -defaultLocation $(defaultLocation) -databricksWorkspace $(databricksWorkspace) -databricksSKU $(databricksSKU)'

- task: Bash@3
  inputs:
    targetType: 'inline'
    script: |
      #!/bin/bash

      HOST=https://adb-490638034087336.16.azuredatabricks.net/
      TOKEN=dapie49f2228797aa96523e4ae22a7049b50
      if [[ -z $HOST || -z $TOKEN ]]
      then
          echo 'The Databricks host URL and secret access token must be passed from job VM'
          exit 1
      fi

      pip install wheel
      python -m pip install databricks-cli
      echo $HOME
      echo -e "[DEFAULT]\nhost: $HOST\ntoken: $TOKEN" > $HOME/.databrickscfg
      echo -e "Testing the conncection - listing dbfs:/"
      #dbfs ls
- task: PythonScript@0
  inputs:
    scriptSource: 'inline'
    script: |
      import requests
      
      DOMAIN = 'adb-490638034087336.16.azuredatabricks.net/'
      TOKEN = 'dapie49f2228797aa96523e4ae22a7049b50'
      
      response = requests.post(
        'https://%s/api/2.0/clusters/create' % (DOMAIN),
        headers={'Authorization': 'Bearer %s' % TOKEN},
        json=
          {
              "num_workers": 0,
              "cluster_name": "TescoGazdasagosCluster",
              "spark_version": "7.4.x-scala2.12",
              "spark_conf": {
                  "spark.master": "local[*]",
                  "spark.databricks.cluster.profile": "singleNode"
              },
              "node_type_id": "Standard_DS3_v2",
              "ssh_public_keys": [],
              "custom_tags": {
                  "ResourceClass": "SingleNode"
              },
              "spark_env_vars": {
                  "PYSPARK_PYTHON": "/databricks/python3/bin/python3"
              },
              "autotermination_minutes": 10,
              "cluster_source": "UI",
              "init_scripts": []
          }
      )
      if response.status_code == 200:
        print(response.json()['cluster_id'])
      else:
        print("Error launching cluster: %s: %s" % (response.json()["error_code"], response.json()["message"]))

- task: Bash@3
  inputs:
    targetType: 'inline'
    script: |
      #!/bin/bash
      
      # modify the configuration JSON with an environment suffix for the cluster name
      # note that the pipeline changes into the directory of this script
      
      echo "Creating Cluster"
      CLUSTER_ID=$(databricks clusters create --json-file config.cluster.json | jq -r '.cluster_id')
      
      