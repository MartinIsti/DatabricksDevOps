# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

pool:
  vmImage: 'ubuntu-latest'

variables:
- group: EnvironmentVariables

steps:
- script: echo $(keyVaultName)
  displayName: 'Show variable(s)'

#- script: az --version
#  displayName: 'Show Azure CLI version'
  
#- script: az upgrade --yes
#  displayName: 'Upgrade Azure CLI version if needed'

#- task: AzureCLI@2
#  inputs:
#    azureSubscription: 'ARM'
#    scriptType: 'pscore'
#    scriptLocation: 'scriptPath'
#    scriptPath: '01 - ResourceGroup.ps1'
#    arguments: '-rgName $(rgName) -defaultLocation $(defaultLocation)'

# - task: AzureCLI@2
#   inputs:
#     azureSubscription: 'ARM'
#     scriptType: 'pscore'
#     scriptLocation: 'scriptPath'
#     scriptPath: '02 - StorageAccount.ps1'
#     arguments: '-rgName $(rgName) -storageAccountName $(storageAccountName)'
# - task: AzureCLI@2
#   inputs:
#     azureSubscription: 'ARM'
#     scriptType: 'pscore'
#     scriptLocation: 'scriptPath'
#     scriptPath: '03 - KeyVault.ps1'
#     arguments: '-rgName $(rgName) -keyVaultName $(keyVaultName)'
# - task: AzureCLI@2
#   inputs:
#     azureSubscription: 'ARM'
#     scriptType: 'pscore'
#     scriptLocation: 'scriptPath'
#     scriptPath: '04 - AzureDataFactory.ps1'
#     arguments: '-rgName $(rgName) -ADFName $(ADFName)'
# - task: AzureCLI@2
#   inputs:
#     azureSubscription: 'ARM'
#     scriptType: 'pscore'
#     scriptLocation: 'scriptPath'
#     scriptPath: '05 - AzureDatabricks.ps1'
#     arguments: '-rgName $(rgName) -defaultLocation $(defaultLocation) -databricksWorkspace $(databricksWorkspace) -databricksSKU $(databricksSKU)'

#- task: Bash@3
#  inputs:
#    targetType: 'inline'
#    script: |
#      pip install wheel
#
#      python -m pip install databricks-cli
- task: Bash@3
  inputs:
    targetType: 'inline'
    script: |
      #!/bin/bash
      
      cat config.cluster.json | sed "s/CLUSTER_NAME/MI_CLUSTER/g" > /tmp/conf.json
      # modify the configuration JSON with an environment suffix for the cluster name
      # note that the pipeline changes into the directory of this script
      
      echo "Creating Cluster"
      CLUSTER_ID=$(databricks clusters create --json-file /tmp/conf.json | jq -r '.cluster_id')
      
      STATE=$(databricks clusters list --output json | jq -r --arg I "$CLUSTER_ID" '.clusters[] | select(.cluster_id == $I) | .state')
      
      echo "Wait for cluster to be PENDING"
      while [[ "$STATE" != "PENDING" ]]
      do
          STATE=$(databricks clusters list --output json | jq -r --arg I "$CLUSTER_ID" '.clusters[] | select(.cluster_name == $I) | .state')
      done
      
      # the API is flaky and the library install complains about terminated clusters
      # so wait a bit more before continuing task
      sleep 10
      
      echo "Cluster $CLUSTER_ID is pending"